+++
author = "Samuel Barreto"
date = "2016-10-03T01:30:04+02:00"
math = false
meta = true
title = "Approches"
toc = true

+++

Quelles sont les deux approches envisagées à ce jour pour étudier la
conversion génique _in-vitro_ ?

<!--more-->

Approche locus dépendante
=========================

{{< figure
src="/img/construction-insertion-simple.svg"
>}}

C'est notre approche classique jusqu'à maintenant. L'idée est
d'adapter le protocole pour pouvoir passer à l'échelle supérieure et
séquencer un grand nombre de clones, à un prix raisonnable, en
attendant un signal suffisamment fort, et en tout cas suffisamment
puissant pour pouvoir exclure "l'absence de preuve plutôt que la
preuve d'absence" (au cas où…).

Ça implique aussi de construire de nouvelles séquences de synthèse
pour chaque modèle, locus ou souche.

## Séquencer en Sanger ?

Ça on sait bien le faire. C'est relativement léger en manip, c'est
déjà fait, déjà validé. Le principal inconvénient est le coût. Si on
veut avoir suffisamment de puissance, le séquençage de sanger est à
300€ par plaque. D'après les calculs de puissance que j'avais fait
pour le rapport de stage^[Commit du thu may 26.], il faudrait séquencer 717 clones pour
détecter une différence de l'ordre de 30 bp entre les longueurs de
régions converties (une base de plus insérée dans un groupe que dans
l'autre en moyenne). Si on s'intéresse aux restaurations ponctuelles,
il faudrait séquencer 2200 clones pour pouvoir observer un effet
faible du biais de conversion avec un niveau de puissance convenable
(0,8).

## Séquencer en Illumina ?

On peut avoir différentes stratégies pour séquencer en short-reads.
La première serait de ne cibler qu'une petite partie du locus
correspondant à la partie comprise entre le flag et une amorce
localisée au sein de la séquence de synthèse.

Construction : gs + flag + kana + ancre.
Le problème le plus important est la validité de la diversité des
flags. Les gens de thermo fischer pourraient peut être construire la
séquence entière, d'après leur commercial. Par contre le coût de la
séquence de synthèse risque d'être assez élevé, puisqu'on aurait une
longueur totale de 3kb environ, incluant des flags.

L'intégration dans le génome d'_A.baylyi_ se ferait par les mêmes
procédés qu'utilisés jusqu'à maintenant. À l'issue de la phase
d'intégration (2h à 30°C), on peut avoir deux stratégies. 1) On ajoute
de la kanamycine dans la suspension, en milieu liquide. 2) On étale
les colonies sur milieu solide + kana. Le problème de 1) est qu'on
peut avoir un clone qui envahit toute la culture relativement
rapidement (biblio ?). Le problème de 2) est qu'on est obligé de
purifier les clones un par un, soit beaucoup — beaucoup — de travail
et de manipulations avant le séquençage, donc d'autant plus de risques
d'introduire des mutations spontanées.

À l'issue de 1), il faut d'une manière ou d'une autre augmenter la représentativité de la séquence d'intérêt dans le pool d'ADN. On peut alors utiliser des enzymes de restriction, à condition d'être sûr de la spécificité de la restriction {{% sidenote "1" %}}On ne coupe bien que la séquence qui nous intéresse ?{{% /sidenote %}}. On peut aussi amplifier le "metagène", autrement dit amplifier le locus sur l'ensemble de l'ADN total extrait de la suspension liquide. Il se peut qu'on introduise des biais de PCR {{% sidenote "2" %}}Certaines séquences s'amplifient mieux que d'autres ? Certains locus sont plus accessibles que d'autres ?{{% /sidenote %}}

À l'issue de 2) {{% sidenote "3" %}}en milieu solide{{% /sidenote %}}, on peut amplifier individuellement chaque clone comme on le fait déjà. Ça permettrait de quantifier pour équilibrer la représentativité des clones et construire un pool d'amplicon, qui peut ensuite être envoyé à séquencer.

Avantage : on garde la stratégie existante, pour lesquelles les manips
sont connues.

Le principal verrou de cet approche est qu'on ne peut séquencer qu'une
petite partie du locus en NGS. L'approche NGS n'est pas vraiment
adaptée à ce genre de problème. On peut éventuellement adapter des
protocoles de métagénomique (le séquençage d'amplicon est souvent
utilisé pour des pools d' ADN 16S.). Le problème suivant est que la —
très — faible divergence entre nos reads conduirait à un signal très
bruité : toutes les caméras flashent en même temps les mêmes reads.
S'il n'y a pas suffisamment de divergence entre les reads, les caméras
sont saturées, et on perd la moitié de la flow cell en solution
calibrante Illumina qui vaut cher.

## Séquencer en nanopore ?

Une approche éventuellement intéressante serait de tester les
nouvelles générations de séquençage nanopore.

https://nanoporetech.com/applications/dna-nanopore-sequencing

On n'est pas limité par la taille des reads (512kb déjà rapportés…).
On peut passer l'ensemble des reads avec une seule flow cell
([@edwards_extreme_2016]). C'est du brin par brin, donc pas de soucis
d'homologie etc… C'est pas cher (starter kits à 1k€ devrait d'après
mes quelques recherches suffire à passer l'ensemble des lectures des
manips). Et on peut le faire au labo (on aurait le séquenceur au
labo).

Inconvénient : c'est nouveau, encore peu connu (mais
de mieux en mieux), peu de recul sur la fiabilité et le taux d'erreur
(possibilité de séquencer en fwd-rev pour diminuer le taux d'erreur).
Nécessité formation ou en tout cas de bien préparer ça.



Approche globale
================

Cette approche s'inspire de celle utilisée par Bubendorfer *et al* dans
le papier de 2016 sur *H.pylori*.

Dans les paragraphes suivants, je développe les différentes étapes du
protocole qui nous intéressent et pourraient être appliquées à notre
projet.

Pré-requis : déterminer deux souches divergentes d'*Acinetobacter baylyi*.
--------------------------------------------------------------------------

Ils utilisent deux souches d'*H.pylori* qui divergent de 4.5%, donc en
moyenne un polymorphisme toutes les 22 paires de bases.

Notre capacité à déterminer avec précision les régions converties dépend
de la fréquence de divergence entre le donneur et le receveur.

Intégration passive : expériences de transformations répétées
-------------------------------------------------------------

L'idée est de procéder à des transformations successives.

{{< figure
src="/img/transformation-repetee.svg"
type="full"
>}}


Toutes les cultures s'effectuent en milieu sélectif. Il faut quand même
contrôler régulièrement pour l'absence de contaminations. À chaque
étape, plusieurs aliquots peuvent être utilisés pour assurer un backup.

À la fin des transformations répétées, l'aliquot est étalé sur un milieu
non sélectif. Ils ont séquencé 20 clones, plus deux transformés
uniquement par de l'eau (*ie* incubés en présence d'eau au lieu d'ADN de
la souche donneuse). Le séquençage se fait en fragmentant le génome des
souches par un kit Illumina, poolé et séquencé en MiSeq.

Cette manip nécessite d'avoir une importante quantité d'ADN de souche
donneuse.

Intégration forcée : expériences de transformations simples
-----------------------------------------------------------

Même protocole qu'auparavant.

La souche receveuse est incubée en présence d'un excès d'ADN de la
souche donneuse, puis étalée au bout d'une génération sur un milieu
sélectionnant les intégrations de la cassette de résistance au
chloramphénicol. L'intégration de cette cassette passe par deux régions
identiques au génome receveur en 5' et 3' de la cassette.

Que disent leurs résultats ?
----------------------------

Ils ont pu observer :

-   la distribution des évènements de recombinaison à l'échelle du
    génome chez *H.pylori*.
-   la distribution de la longueur des évènements de recombinaison :
    combien d'imports par souche ? quelle longueur ? combien de
    remplacements ?

Ils ont montré que la distribution de la longueur des évènements de
recombinaison dans ce modèle est bimodale. Les évènements sont soit
très courts (insertion de 28bp en moyenne), soit longs (1645bp). Cette
approche est suffisamment puissante pour pouvoir analyser à la fois
les deux types d'insertions. L'approche locus dépendante a pour
inconvénient de biaiser l'observation : on n'observe que des
évènements d'une longueur comprise entre 0 et la longueur maximale de
la région intégrée.

Un de leur résultat anodin mais important pour nous est que la
longueur des évènements sélectionnés (*ie* associés à une cassette de
résistance) est plus importante que la longueur des évènements
d'intégration "passive". La plupart des évènements d'intégration
non-sélectionnés dans leurs manips d'intégrations forcée sont à
proximité du site d'intégration de la cassette, indiquant une liaison
entre l'évènement de recombinaison sélectionné et ceux
non-sélectionnés.
